{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBbQcw1oUnRj",
        "outputId": "1774ff33-9cbb-4a38-c14f-df55c51f269b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install nltk tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Installing collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.9.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8GM0CoNU7CR",
        "outputId": "58ff0870-406d-4e64-bd41-6fb128238ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94uh8a_dTK8a",
        "outputId": "fd6a6bd7-8756-4271-a9b4-0382e9203a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf csci-ling-5832-project\n",
        "!git clone https://github.com/mease/csci-ling-5832-project.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'csci-ling-5832-project'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 157 (delta 29), reused 42 (delta 17), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (157/157), 79.48 MiB | 27.62 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Checking out files: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzdHEReXT2oC",
        "outputId": "7d04bcad-767c-47d9-e599-f68649237364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls csci-ling-5832-project/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "da-en  de-en  fr-en  it-en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X9JW5Ew3eVw"
      },
      "source": [
        "Mount your Google Drive for saving logs and checkpoints. Files save in the Colab environment are deleted when Colab exits, so we need to save them to a Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Wk7MjnmzxY",
        "outputId": "45eca65a-78ed-47d7-dd72-82bb9f3efd8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4Dl_uC5LDX",
        "outputId": "4932a079-df23-4ebc-cf0f-3e5cb748c50c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/nlp\n",
        "!ln -s /content/drive/My\\ Drive/nlp csci-ling-5832-project/nlp\n",
        "!cd csci-ling-5832-project && chmod 777 default_train.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/My Drive/nlp’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlXqsH2DEvyP",
        "outputId": "374c1355-6dc1-40a9-d220-7f123df46009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cd csci-ling-5832-project && python train.py -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [--data_path DATA_PATH] [--learning_rate LEARNING_RATE]\n",
            "                [--d_model D_MODEL] [--nhead NHEAD]\n",
            "                [--num_encoder_layers NUM_ENCODER_LAYERS]\n",
            "                [--num_decoder_layers NUM_DECODER_LAYERS]\n",
            "                [--dim_feedforward DIM_FEEDFORWARD] [--dropout DROPOUT]\n",
            "                [--checkpoint_file CHECKPOINT_FILE] [--log_file LOG_FILE]\n",
            "                [--do_train DO_TRAIN]\n",
            "                train_file val_file test_file num_epochs batch_size tokenizer\n",
            "                src_tok_file tgt_tok_file\n",
            "\n",
            "positional arguments:\n",
            "  train_file            Training language pair file (tab-separated).\n",
            "  val_file              Validation language pair file (tab-separated).\n",
            "  test_file             Validation language pair file (tab-separated).\n",
            "  num_epochs            Number of epochs to train.\n",
            "  batch_size            Batch size.\n",
            "  tokenizer             The tokenizer type.\n",
            "  src_tok_file          The source tokenizer file.\n",
            "  tgt_tok_file          The target tokenizer file.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_path DATA_PATH\n",
            "                        Directory containing the training files.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        Adam optimizer learning rate.\n",
            "  --d_model D_MODEL     The number of expected features in the input to the\n",
            "                        transformer.\n",
            "  --nhead NHEAD         The number of attention heads.\n",
            "  --num_encoder_layers NUM_ENCODER_LAYERS\n",
            "                        The number of encoder layers.\n",
            "  --num_decoder_layers NUM_DECODER_LAYERS\n",
            "                        The number of decoder layers.\n",
            "  --dim_feedforward DIM_FEEDFORWARD\n",
            "                        The dimension of the feedforward network model.\n",
            "  --dropout DROPOUT     The dropout value.\n",
            "  --checkpoint_file CHECKPOINT_FILE\n",
            "                        The file to save model checkpoint to.\n",
            "  --log_file LOG_FILE   The file to write logs to.\n",
            "  --do_train DO_TRAIN   If False, skip training and only do scoring from\n",
            "                        checkpoint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUA6jXy31hIz"
      },
      "source": [
        "This trains the model using default hyperparameters we decided on. Format is:\n",
        "\n",
        "`./default_train.sh source_language target_language tokenizer epochs batch_size output_path`\n",
        "\n",
        "Languages: da, de, fr, it, en\n",
        "\n",
        "Tokenizers: word, bpe, wordpiece, character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13jtKL2V2RIK",
        "outputId": "ab9abf65-2859-4f8b-e34b-327dde18cd8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cd csci-ling-5832-project && ./default_train.sh de en bpe 1 64 nlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using tokenizer: bpe\n",
            "[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                   0\n",
            "\u001b[1A\r\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                   3\n",
            "\u001b[1A\r\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                   6\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                  10\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  13\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  17\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  21\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  25\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  28\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  32\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  36\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ███░░░░░                  40\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ███░░░░░                  44\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ███░░░░░                  48\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ████░░░░                  52\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ████░░░░                  55\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ████░░░░                  59\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  63\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  67\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  70\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  74\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ██████░░                  78\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ██████░░                  82\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ██████░░                  86\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ███████░                  90\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ███████░                  94\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (10 Mo)                    ███████░                  98\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (10 Mo)                    ████████                 100\n",
            "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           █████░░░ 42884    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 60411    /    60411\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 3020     /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 4832     /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 6040     /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 7248     /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 10268    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 12080    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 13288    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 14496    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 15704    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 18724    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 20536    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██░░░░░░ 21744    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 22952    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 25972    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 27784    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 28992    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 30200    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 32616    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 34428    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 35636    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 36844    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █████░░░ 38656    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █████░░░ 41072    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              █████░░░ 42884    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              █████░░░ 44092    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              █████░░░ 45300    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 47112    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 49528    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 50736    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 51944    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 53756    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 56172    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 57984    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 59192    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 60400    /    60411\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████████ 60411    /    60411\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 200      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 400      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 800      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1600     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           █░░░░░░░ 2800     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██░░░░░░ 5200     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ███░░░░░ 8400     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████░░░░ 12400    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██████░░ 16400    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 19737    /    19737\n",
            "\n",
            "[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   4\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   8\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                  12\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     █░░░░░░░                  16\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     █░░░░░░░                  20\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     █░░░░░░░                  24\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  28\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  32\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  35\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ███░░░░░                  39\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ███░░░░░                  43\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ███░░░░░                  47\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  50\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  54\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  58\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  61\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     █████░░░                  64\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     █████░░░                  68\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     █████░░░                  72\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  76\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  80\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  84\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ███████░                  88\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ███████░                  92\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (9 Mo)                     ███████░                  96\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (9 Mo)                     ████████                 100\n",
            "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 27886    /    27886\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 6394     /    27886\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 12510    /    27886\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 17236    /    27886\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██████░░ 23630    /    27886\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 27886    /    27886\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 200      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1600     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██░░░░░░ 5200     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████░░░░ 11200    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ███████░ 18400    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 19737    /    19737\n",
            "\n",
            "Loading training data...\n",
            "SRC vocab size: 18689\n",
            "TGT vocab size: 17245\n",
            "Starting Epoch [1/1]\n",
            "Epoch [1/1] complete in 788.589 seconds.\n",
            "Train Loss: 104.971. Val Loss: 89.557\n",
            "Saving state dict\n",
            "\n",
            "Source: in den betrieben ist die iso 9000 schon selbstverstandlich geworden .\n",
            "\n",
            "Target: iso 9000 is already taken for granted in firms .\n",
            "\n",
            "Predicted: the european union is not a number of member states to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to\n",
            "-----------------------------------------\n",
            "\n",
            "Training complete in 0.220 hours.\n",
            "Best train loss: 104.97089617357602. Best val loss: 89.55696671448707. Attained at epoch 1\n",
            "\n",
            "Loading best model.\n",
            "Scoring the test set...\n",
            "Scoring complete in 239.484 minutes.\n",
            "BLEU : 0.43327520273345005\n",
            "CHRF : 0.23984176293164808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqrEDA5p0baE"
      },
      "source": [
        "Example of running the training script (uncomment to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poI-Z3NrTp0Q"
      },
      "source": [
        "#!cd csci-ling-5832-project && python train.py de-en-train.txt de-en-val.txt de-en-test.txt 1 64 bpe data/de-en/de-tok.txt data/de-en/en-tok.txt --data_path data/de-en --d_model 512 --nhead 8 --num_encoder_layers 6 --num_decoder_layers 6 --learning_rate 0.0001 --dropout 0.1 --checkpoint_file /content/drive/My\\ Drive/nlp/en-de-bpe.ck --log_file /content/drive/My\\ Drive/nlp/en-de-bpe.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePhcTMSy0pTd"
      },
      "source": [
        "Example of scoring the test set against an existing checkpoint. All settings must match EXACTLY the same settings you used to train the checkpoint. (uncomment to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXxvG_hj3Ovg"
      },
      "source": [
        "#!cd csci-ling-5832-project && python train.py de-en-train.txt de-en-val.txt de-en-test.txt 1 64 bpe data/de-en/de-tok.txt data/de-en/en-tok.txt --data_path data/de-en --d_model 512 --nhead 8 --num_encoder_layers 6 --num_decoder_layers 6 --learning_rate 0.0001 --dropout 0.1 --checkpoint_file /content/drive/My\\ Drive/nlp/en-de-bpe.ck --log_file /content/drive/My\\ Drive/nlp/en-de-bpe-rescore.log --do_train False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeVK21ez5sTx"
      },
      "source": [
        "Flush the Google Drive to ensure things get saved. Terminate the colab session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuQhGdn4owSg"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik2fLhQqKqQ2"
      },
      "source": [
        "# This will terminate the Colab session\n",
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}