{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBbQcw1oUnRj",
        "outputId": "16890898-c9f1-4061-bc27-0633ed3787ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "! pip install nltk tokenizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8GM0CoNU7CR",
        "outputId": "0ac7aba5-dfbe-457f-f26b-8c4b00a43aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94uh8a_dTK8a",
        "outputId": "ce1b9024-a0e8-428e-ab1e-9047bbda22f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "!rm -rf csci-ling-5832-project\n",
        "!git clone https://github.com/mease/csci-ling-5832-project.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'csci-ling-5832-project'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 49 (delta 22), reused 41 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzdHEReXT2oC",
        "outputId": "470af1fc-7c11-41b1-b400-003381348d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls csci-ling-5832-project/data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "de-en-test.txt\t de-en-val.txt\ten-de-test.txt\t en-de-val.txt\n",
            "de-en-train.txt  de-tok.txt\ten-de-train.txt  en-tok.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlXqsH2DEvyP",
        "outputId": "497f9e6a-6ef0-49ad-e4e9-772bd43899fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "!cd csci-ling-5832-project && python train.py -h"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [--data_path DATA_PATH] [--learning_rate LEARNING_RATE]\n",
            "                [--d_model D_MODEL] [--nhead NHEAD]\n",
            "                [--num_encoder_layers NUM_ENCODER_LAYERS]\n",
            "                [--num_decoder_layers NUM_DECODER_LAYERS]\n",
            "                [--dim_feedforward DIM_FEEDFORWARD] [--dropout DROPOUT]\n",
            "                train_file val_file num_epochs batch_size tokenizer\n",
            "                src_tok_file tgt_tok_file\n",
            "\n",
            "positional arguments:\n",
            "  train_file            Training language pair file (tab-separated).\n",
            "  val_file              Validation language pair file (tab-separated).\n",
            "  num_epochs            Number of epochs to train.\n",
            "  batch_size            Batch size.\n",
            "  tokenizer             The tokenizer type.\n",
            "  src_tok_file          The source tokenizer file.\n",
            "  tgt_tok_file          The target tokenizer file.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_path DATA_PATH\n",
            "                        Directory containing the training files.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        Adam optimizer learning rate.\n",
            "  --d_model D_MODEL     The number of expected features in the input to the\n",
            "                        transformer.\n",
            "  --nhead NHEAD         The number of attention heads.\n",
            "  --num_encoder_layers NUM_ENCODER_LAYERS\n",
            "                        The number of encoder layers.\n",
            "  --num_decoder_layers NUM_DECODER_LAYERS\n",
            "                        The number of decoder layers.\n",
            "  --dim_feedforward DIM_FEEDFORWARD\n",
            "                        The dimension of the feedforward network model.\n",
            "  --dropout DROPOUT     The dropout value.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poI-Z3NrTp0Q",
        "outputId": "33fb8c3f-d23a-44e7-bdf9-e28cc22d3b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd csci-ling-5832-project && python train.py de-en-train.txt de-en-val.txt 1 64 bpe data/de-tok.txt data/en-tok.txt --d_model 512 --nhead 8 --num_encoder_layers 6 --num_decoder_layers 6 --learning_rate 0.0001 --dropout 0.1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using tokenizer: bpe\n",
            "[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                   0\n",
            "\u001b[1A\r\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                   3\n",
            "\u001b[1A\r\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                   7\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ░░░░░░░░                  10\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  13\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  16\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  19\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    █░░░░░░░                  23\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  26\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  30\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  33\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ██░░░░░░                  37\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ███░░░░░                  40\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (10 Mo)                    ███░░░░░                  44\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ███░░░░░                  48\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ████░░░░                  52\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ████░░░░                  56\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ████░░░░                  60\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  64\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  68\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    █████░░░                  72\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ██████░░                  76\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ██████░░                  80\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ██████░░                  84\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (10 Mo)                    ███████░                  88\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (10 Mo)                    ███████░                  92\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (10 Mo)                    ███████░                  95\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (10 Mo)                    ███████░                  99\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (10 Mo)                    ████████                 100\n",
            "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           █████░░░ 39999    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 59799    /    59799\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 2985     /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 4776     /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 5970     /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 7164     /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 10149    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 11940    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 13134    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 14328    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 16119    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 18507    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 19701    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 20895    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██░░░░░░ 22089    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 24477    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 26268    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 27462    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 28656    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███░░░░░ 29850    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 32238    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 34029    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 35223    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████░░░░ 36417    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █████░░░ 37611    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █████░░░ 39999    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █████░░░ 41790    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              █████░░░ 42984    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              █████░░░ 44178    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 45969    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 48357    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 50148    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██████░░ 51342    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 52536    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 54924    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 56715    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 57909    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 59103    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███████░ 59700    /    59799\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████████ 59799    /    59799\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 200      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 400      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 800      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1600     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           █░░░░░░░ 2800     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██░░░░░░ 5000     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ███░░░░░ 8200     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████░░░░ 11800    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██████░░ 15800    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 19741    /    19741\n",
            "\n",
            "[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   3\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   6\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ░░░░░░░░                   9\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     █░░░░░░░                  13\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     █░░░░░░░                  17\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     █░░░░░░░                  21\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  25\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  29\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  33\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ██░░░░░░                  37\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ███░░░░░                  41\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files (9 Mo)                     ███░░░░░                  45\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ███░░░░░                  49\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  52\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  55\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ████░░░░                  59\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     █████░░░                  63\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     █████░░░                  67\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     █████░░░                  71\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  75\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  79\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  83\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ██████░░                  87\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files (9 Mo)                     ███████░                  91\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (9 Mo)                     ███████░                  94\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (9 Mo)                     ███████░                  98\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Reading files (9 Mo)                     ████████                 100\n",
            "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 27807    /    27807\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 6116     /    27807\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 11954    /    27807\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 16958    /    27807\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██████░░ 22518    /    27807\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███████░ 27800    /    27807\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 27807    /    27807\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 200      /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1400     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██░░░░░░ 5400     /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████░░░░ 10600    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██████░░ 17400    /    20000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 19741    /    19741\n",
            "\n",
            "Loading training data...\n",
            "SRC vocab size: 18691\n",
            "TGT vocab size: 17189\n",
            "Starting Epoch [1/1]\n",
            "Epoch [1/1] complete in 566.837 seconds.\n",
            "Train Loss: 104.596. Val Loss: 90.241\n",
            "Saving state dict\n",
            "\n",
            "Source: bis vor kurzem leisteten 29 europaische nro humanitare hilfe in afghanistan .\n",
            "\n",
            "Target: in fact we had 29 european ngos delivering humanitarian aid inside afghanistan .\n",
            "\n",
            "Predicted: the same time , the same-term countries , on the same-term countries , on the same states , on the same states , on the same states .</s>\n",
            "-----------------------------------------\n",
            "\n",
            "Training complete in 0.158 hours.\n",
            "Best train loss: 104.5956965035837. Best val loss: 90.24132721716404. Attained at epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik2fLhQqKqQ2"
      },
      "source": [
        "# This will terminate the Colab session\n",
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}